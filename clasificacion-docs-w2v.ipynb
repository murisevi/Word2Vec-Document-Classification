{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e0e7a2",
   "metadata": {},
   "source": [
    "# Clasificación de documentos con word2vec\n",
    "### Dpto. Ciencias de la Computación e Inteligencia Artificial - Universidad de Sevilla\n",
    "Realizado por Lucas Antoñanzas del Villar (lucantdel) y Jaime García García (jaigagar1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36e028",
   "metadata": {},
   "source": [
    "## Construcción del corpus\n",
    "En este apartado, se describe el proceso de construcción del corpus utilizado en nuestro proyecto de clasificación de documentos. Encontramos una [compilación de 40 datasets de noticias gratis](https://www.linkedin.com/pulse/free-news-datasets-mega-compilation-rajat-thakur/) y tras contemplar varias opciones escogimos una [colección de textos periodísticos en inglés de la BBC](http://mlg.ucd.ie/datasets/bbc.html). Esta fuente nos proporciona una variedad de artículos de diferentes categorías, en concreto: business, entertainment, politics, sport y tech.\n",
    "\n",
    "Para obtener el corpus, accedimos al enlace proporcionado y descargamos los archivos de texto agrupados por categorías en diferentes carpetas. Cada carpeta contiene múltiples archivos .txt de una categoría específica, y cada uno de estos, un artículo (título, subtítulo y cuerpo de la noticia).\n",
    "\n",
    "Una vez obtenidos los textos de las diferentes categorías, procedimos a transformar todos estos archivos .txt a un solo archivo .csv con las siguientes columnas: id, article y category. Etiquetamos los artículos según la carpeta a la que pertenecen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb7ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated the .csv file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Directorio raíz donde se encuentran las carpetas por categoría\n",
    "root_directory = \"./bbc-fulltext/bbc\"\n",
    "\n",
    "# Nombre del archivo .csv de salida\n",
    "csv_file = \"./news.csv\"\n",
    "\n",
    "# Lista para almacenar las filas del archivo .csv\n",
    "csv_rows = []\n",
    "\n",
    "# Número por el que comenzarán los id\n",
    "id = 0\n",
    "\n",
    "# Recorre todas las carpetas en el directorio raíz\n",
    "for category in os.listdir(root_directory):\n",
    "    # Obtiene la ruta completa de la carpeta de la categoría\n",
    "    category_folder  = os.path.join(root_directory, category)\n",
    "    \n",
    "    # Verifica que sea una carpeta\n",
    "    if os.path.isdir(category_folder):\n",
    "        # Recorre todos los archivos .txt en la carpeta de la categoría\n",
    "        for txt_file  in os.listdir(category_folder):\n",
    "            # Obtiene la ruta completa del archivo .txt\n",
    "            txt_file_path  = os.path.join(category_folder, txt_file)\n",
    "            \n",
    "            # Lee el contenido del archivo .txt\n",
    "            with open(txt_file_path, 'r') as file:\n",
    "                content  = file.read().replace(\".\\n\\n\", \". \").replace(\"\\n\\n\", \". \").replace(\"\\n\", \"\")\n",
    "                \n",
    "                # Agrega una nueva fila al archivo .csv\n",
    "                row = [id, category, content]\n",
    "                csv_rows.append(row)\n",
    "                id += 1\n",
    "\n",
    "# Escribe las filas en el archivo .csv\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    csv_writer  = csv.writer(file, delimiter=';')\n",
    "    csv_writer.writerow(['id', 'category', 'content'])\n",
    "    csv_writer.writerows(csv_rows)\n",
    "\n",
    "print(\"Successfully generated the .csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192107b6",
   "metadata": {},
   "source": [
    "#### Análisis del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f62927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport            511\n",
      "business         510\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"news.csv\", sep=';')\n",
    "\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f993eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de filas: 2225\n"
     ]
    }
   ],
   "source": [
    "print(\"Número total de filas:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa1be7",
   "metadata": {},
   "source": [
    "A continuación se muestran 5 filas aleatorias del archivo .csv que hemos generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a646868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>business</td>\n",
       "      <td>Peugeot deal boosts Mitsubishi. Struggling Jap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>1854</td>\n",
       "      <td>tech</td>\n",
       "      <td>Solutions to net security fears. Fake bank e-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>227</td>\n",
       "      <td>business</td>\n",
       "      <td>Tsunami 'to hit Sri Lanka banks'. Sri Lanka's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>business</td>\n",
       "      <td>Market unfazed by Aurora setback. As the Auror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>541</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Redford's vision of Sundance. Despite sporting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       category                                            content\n",
       "12      12       business  Peugeot deal boosts Mitsubishi. Struggling Jap...\n",
       "1854  1854           tech  Solutions to net security fears. Fake bank e-m...\n",
       "227    227       business  Tsunami 'to hit Sri Lanka banks'. Sri Lanka's ...\n",
       "243    243       business  Market unfazed by Aurora setback. As the Auror...\n",
       "541    541  entertainment  Redford's vision of Sundance. Despite sporting..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_index = random.sample(range(0, df.shape[0]), 5)\n",
    "df.iloc[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f179d4",
   "metadata": {},
   "source": [
    "## Tokenización de texto y eliminación de 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecf07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Elimina todo lo que no sean letras y lo sustituye por espacios\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # Pasa las palabras a minúsculas y separa las palabras\n",
    "    tokens = review_text.lower().split()\n",
    "    \n",
    "    # Elimina los caracteres sueltos\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    # Elimina las stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d9499",
   "metadata": {},
   "source": [
    "A continuación, mostraremos un articulo del csv y el resultado de tokenizarlo y eliminar las stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef6c482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dollar gains on Greenspan speech. The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise. And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\". Worries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n"
     ]
    }
   ],
   "source": [
    "article = df['content'].tolist()[1]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760b29cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dollar', 'gains', 'greenspan', 'speech', 'dollar', 'hit', 'highest', 'level', 'euro', 'almost', 'three', 'months', 'federal', 'reserve', 'head', 'said', 'us', 'trade', 'deficit', 'set', 'stabilise', 'alan', 'greenspan', 'highlighted', 'us', 'government', 'willingness', 'curb', 'spending', 'rising', 'household', 'savings', 'factors', 'may', 'help', 'reduce', 'late', 'trading', 'new', 'york', 'dollar', 'reached', 'euro', 'thursday', 'market', 'concerns', 'deficit', 'hit', 'greenback', 'recent', 'months', 'friday', 'federal', 'reserve', 'chairman', 'mr', 'greenspan', 'speech', 'london', 'ahead', 'meeting', 'finance', 'ministers', 'sent', 'dollar', 'higher', 'earlier', 'tumbled', 'back', 'worse', 'expected', 'us', 'jobs', 'data', 'think', 'chairman', 'taking', 'much', 'sanguine', 'view', 'current', 'account', 'deficit', 'taken', 'time', 'said', 'robert', 'sinche', 'head', 'currency', 'strategy', 'bank', 'america', 'new', 'york', 'taking', 'longer', 'term', 'view', 'laying', 'set', 'conditions', 'current', 'account', 'deficit', 'improve', 'year', 'next', 'worries', 'deficit', 'concerns', 'china', 'however', 'remain', 'china', 'currency', 'remains', 'pegged', 'dollar', 'us', 'currency', 'sharp', 'falls', 'recent', 'months', 'therefore', 'made', 'chinese', 'export', 'prices', 'highly', 'competitive', 'calls', 'shift', 'beijing', 'policy', 'fallen', 'deaf', 'ears', 'despite', 'recent', 'comments', 'major', 'chinese', 'newspaper', 'time', 'ripe', 'loosening', 'peg', 'meeting', 'thought', 'unlikely', 'produce', 'meaningful', 'movement', 'chinese', 'policy', 'meantime', 'us', 'federal', 'reserve', 'decision', 'february', 'boost', 'interest', 'rates', 'quarter', 'point', 'sixth', 'move', 'many', 'months', 'opened', 'differential', 'european', 'rates', 'half', 'point', 'window', 'believe', 'could', 'enough', 'keep', 'us', 'assets', 'looking', 'attractive', 'could', 'help', 'prop', 'dollar', 'recent', 'falls', 'partly', 'result', 'big', 'budget', 'deficits', 'well', 'us', 'yawning', 'current', 'account', 'gap', 'need', 'funded', 'buying', 'us', 'bonds', 'assets', 'foreign', 'firms', 'governments', 'white', 'house', 'announce', 'budget', 'monday', 'many', 'commentators', 'believe', 'deficit', 'remain', 'close', 'half', 'trillion', 'dollars']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(tokenize_and_remove_stopwords(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b33aa",
   "metadata": {},
   "source": [
    "## Stemming con NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebbb71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Definir el stemmer a utilizar\n",
    "stemmer = SnowballStemmer('english')  # Utiliza SnowballStemmer para español, puedes cambiarlo según el idioma\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenización: utilizamos la función creada en el apartado anterior\n",
    "    tokens = tokenize_and_remove_stopwords(text)\n",
    "    \n",
    "    # Aplicar stemming\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c89a3c",
   "metadata": {},
   "source": [
    "Este es el resultado de aplicar stemming a los tokens del articulo de ejemplo del apartado anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93f99a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dollar', 'gain', 'greenspan', 'speech', 'dollar', 'hit', 'highest', 'level', 'euro', 'almost', 'three', 'month', 'feder', 'reserv', 'head', 'said', 'us', 'trade', 'deficit', 'set', 'stabilis', 'alan', 'greenspan', 'highlight', 'us', 'govern', 'willing', 'curb', 'spend', 'rise', 'household', 'save', 'factor', 'may', 'help', 'reduc', 'late', 'trade', 'new', 'york', 'dollar', 'reach', 'euro', 'thursday', 'market', 'concern', 'deficit', 'hit', 'greenback', 'recent', 'month', 'friday', 'feder', 'reserv', 'chairman', 'mr', 'greenspan', 'speech', 'london', 'ahead', 'meet', 'financ', 'minist', 'sent', 'dollar', 'higher', 'earlier', 'tumbl', 'back', 'wors', 'expect', 'us', 'job', 'data', 'think', 'chairman', 'take', 'much', 'sanguin', 'view', 'current', 'account', 'deficit', 'taken', 'time', 'said', 'robert', 'sinch', 'head', 'currenc', 'strategi', 'bank', 'america', 'new', 'york', 'take', 'longer', 'term', 'view', 'lay', 'set', 'condit', 'current', 'account', 'deficit', 'improv', 'year', 'next', 'worri', 'deficit', 'concern', 'china', 'howev', 'remain', 'china', 'currenc', 'remain', 'peg', 'dollar', 'us', 'currenc', 'sharp', 'fall', 'recent', 'month', 'therefor', 'made', 'chines', 'export', 'price', 'high', 'competit', 'call', 'shift', 'beij', 'polici', 'fallen', 'deaf', 'ear', 'despit', 'recent', 'comment', 'major', 'chines', 'newspap', 'time', 'ripe', 'loosen', 'peg', 'meet', 'thought', 'unlik', 'produc', 'meaning', 'movement', 'chines', 'polici', 'meantim', 'us', 'feder', 'reserv', 'decis', 'februari', 'boost', 'interest', 'rate', 'quarter', 'point', 'sixth', 'move', 'mani', 'month', 'open', 'differenti', 'european', 'rate', 'half', 'point', 'window', 'believ', 'could', 'enough', 'keep', 'us', 'asset', 'look', 'attract', 'could', 'help', 'prop', 'dollar', 'recent', 'fall', 'part', 'result', 'big', 'budget', 'deficit', 'well', 'us', 'yawn', 'current', 'account', 'gap', 'need', 'fund', 'buy', 'us', 'bond', 'asset', 'foreign', 'firm', 'govern', 'white', 'hous', 'announc', 'budget', 'monday', 'mani', 'comment', 'believ', 'deficit', 'remain', 'close', 'half', 'trillion', 'dollar']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_text(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547865b9",
   "metadata": {},
   "source": [
    "A continuación, crearemos el vocabulario final en función de la frecuencia de las palabras que aparecen en el corpus preprocesado. Podemos establecer un umbral para filtrar las palabras de baja frecuencia y considerar solo aquellas que aparecen con mayor regularidad. Esta selección nos permitirá reducir la dimensionalidad y centrarnos en las palabras más informativas para la clasificación de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31c046db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Definir un umbral para filtrar palabras de baja frecuencia\n",
    "threshold = 10\n",
    "\n",
    "# Creación del corpus y vocabulario de todos los artículos\n",
    "corpus = [preprocess_text(text) for text in df['content'].tolist()]\n",
    "all_words = [word for doc in corpus for word in doc]\n",
    "all_words_freq = Counter(all_words)\n",
    "vocabulary = [word for word, freq in all_words_freq.items() if freq >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb074f4",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec es un algoritmo de aprendizaje automático utilizado para representar palabras como vectores numéricos en un espacio vectorial. Se basa en la idea de que el significado de una palabra se puede capturar en función de su contexto lingüístico. Word2Vec utiliza una red neuronal para aprender a predecir la probabilidad de que una palabra aparezca en función de las palabras vecinas en un corpus de texto grande. A medida que el modelo se entrena en un corpus, las palabras similares tienden a tener vectores similares en el espacio vectorial. Estos vectores resultantes pueden usarse para medir la similitud entre palabras, realizar operaciones de analogía y mejorar el rendimiento en tareas de procesamiento de lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548f47b",
   "metadata": {},
   "source": [
    "#### Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ec6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Cuenta el número de núcleos de tu ordenador\n",
    "cores = multiprocessing.cpu_count() \n",
    "\n",
    "w2v_model = Word2Vec(min_count=threshold, # min_count = int - Ignora todas las palabras con una frecuencia absoluta total menor que esto - (2, 100)\n",
    "                 window=5, # window = int - La distancia máxima entre la palabra actual y la palabra predicha dentro de una oración. Por ejemplo, \"window\" palabras a la izquierda y \"window\" palabras a la derecha de nuestro objetivo - (2, 10)\n",
    "                 vector_size=200, # size = int - Dimensionalidad de los vectores de características - (50, 300)\n",
    "                 sample=1e-4, # sample = float - Umbral para configurar qué palabras de alta frecuencia se reducen aleatoriamente. Muy influyente - (0, 1e-5)\n",
    "                 alpha=0.03, # alpha = float - Tasa de aprendizaje inicial - (0.01, 0.05)\n",
    "                 min_alpha=0.0007, # min_alpha = float - La tasa de aprendizaje disminuirá linealmente hasta min_alpha a medida que avance el entrenamiento. Para establecerlo: alpha - (min_alpha * epochs) ~ 0.00\n",
    "                 negative=10, # negative = int - Si es > 0, se utilizará muestreo negativo, donde el entero para negative especifica cuántas \"palabras de ruido\" se deben seleccionar. Si se establece en 0, no se utiliza muestreo negativo - (5, 20)\n",
    "                 workers=cores-1) # workers = int - Utiliza esta cantidad de hilos de trabajo para entrenar el modelo (entrenamiento más rápido con máquinas multicore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af241a6",
   "metadata": {},
   "source": [
    "#### Construcción del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2819e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d91818",
   "metadata": {},
   "source": [
    "#### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5d39731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5645065, 9692000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(corpus, \n",
    "            total_examples=w2v_model.corpus_count, # total_examples = int - Contador de oraciones\n",
    "            epochs=20) # epochs = int - Número de iteraciones (épocas) sobre el corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda276e",
   "metadata": {},
   "source": [
    "#### Ejemplos\n",
    "WARNING: Las palabras introducidas como parámetro deben estar contenidas en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "327dd374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.21677113e+00,  2.11862177e-01, -3.19521219e-01, -8.34378749e-02,\n",
       "       -3.13301951e-01, -4.79111284e-01,  3.40746760e-01, -5.43864481e-02,\n",
       "        8.36227648e-03,  8.15962628e-02, -8.82159472e-02, -6.98183417e-01,\n",
       "       -7.97175050e-01,  6.38369858e-01, -4.90571558e-01, -5.69918931e-01,\n",
       "        1.65025249e-01,  7.18296528e-01,  7.53071979e-02, -5.83105445e-01,\n",
       "        3.20539117e-01, -3.32210332e-01,  6.31354809e-01,  6.21958494e-01,\n",
       "        2.38747876e-02, -4.13469225e-01,  6.17894530e-01, -1.67619616e-01,\n",
       "       -7.81229258e-01,  5.28283007e-02,  2.05473527e-01,  6.07173264e-01,\n",
       "        1.40435547e-01,  2.48686999e-01,  3.79380673e-01, -3.75763059e-01,\n",
       "        9.12328899e-01,  4.53429818e-01,  8.28667581e-01,  3.80425632e-01,\n",
       "       -8.82428791e-03,  2.80631837e-02, -3.22194934e-01,  4.64758158e-01,\n",
       "       -6.29261509e-02, -1.36482030e-01, -7.50863075e-01, -3.22237432e-01,\n",
       "       -6.57721400e-01,  3.04776758e-01, -1.06045529e-01,  2.26440355e-01,\n",
       "       -1.13319564e+00, -2.83862591e-01, -6.02267206e-01, -7.17118382e-01,\n",
       "        7.50965118e-01, -1.56384006e-01, -1.13347757e+00, -1.05242997e-01,\n",
       "        3.59799206e-01, -3.17985058e-01, -2.93005526e-01,  8.05330753e-01,\n",
       "        3.01896125e-01, -1.60315394e-01, -3.30290765e-01,  2.78913945e-01,\n",
       "       -1.08089209e-01,  3.57229970e-02,  1.20555854e+00, -2.41612315e-01,\n",
       "        5.59566617e-01,  3.76557559e-01,  1.55203417e-01, -1.10556632e-01,\n",
       "       -1.36058539e-01, -5.47536492e-01, -4.09023166e-01,  9.99391228e-02,\n",
       "        4.75885868e-01, -8.22054684e-01, -1.99830368e-01,  1.81859717e-01,\n",
       "       -3.15024197e-01, -2.55834669e-01, -1.22029774e-01, -1.09872036e-01,\n",
       "        4.66669798e-01,  3.11565965e-01,  2.14657169e-02,  4.77265775e-01,\n",
       "       -4.28026438e-01,  1.03447068e+00,  5.40697634e-01,  7.00837255e-01,\n",
       "        4.25252706e-01,  2.12396622e-01, -2.49306247e-01,  4.15653735e-01,\n",
       "        6.60643205e-02,  8.37266445e-01,  4.90349412e-01,  1.20725071e+00,\n",
       "       -1.43674076e-01, -3.95199656e-02,  4.37277444e-02, -8.58374834e-01,\n",
       "       -6.30332112e-01, -8.04277509e-02,  4.32754904e-01, -3.17065090e-01,\n",
       "       -6.64891243e-01, -3.68323997e-02,  7.55131245e-01, -7.76928067e-02,\n",
       "       -5.00153303e-01, -8.85580122e-01,  4.02446479e-01, -1.49325654e-01,\n",
       "       -4.46168855e-02,  7.03931302e-02, -2.22462356e-01, -7.34363720e-02,\n",
       "       -7.89382383e-02,  5.11777699e-01,  1.01756193e-02, -6.92811489e-01,\n",
       "        5.97583890e-01,  6.26496077e-01, -6.19487688e-02,  9.44073677e-01,\n",
       "       -8.96797255e-02,  9.58335102e-01, -3.89596254e-01, -7.14358032e-01,\n",
       "       -5.02146721e-01, -3.29832882e-02,  4.14653979e-02,  2.43151397e-01,\n",
       "        3.67316306e-02, -7.04019845e-01,  1.49179026e-01,  5.76512292e-02,\n",
       "       -8.82824957e-01,  1.07730460e+00, -3.34358364e-01,  2.88931042e-01,\n",
       "       -2.15560779e-01,  4.18956764e-02,  2.89899872e-05,  4.44463521e-01,\n",
       "       -4.14024234e-01,  3.88196170e-01, -7.93498516e-01,  3.08425009e-01,\n",
       "       -4.33256894e-01,  3.10921669e-01,  1.08225249e-01, -2.48993769e-01,\n",
       "        5.16749978e-01, -1.75767928e-01, -8.44287634e-01,  2.13511989e-01,\n",
       "       -4.82557118e-02, -5.44234514e-01,  2.65469611e-01, -5.77945054e-01,\n",
       "        1.57348961e-02,  1.10051644e+00, -1.13467288e+00,  9.59137321e-01,\n",
       "        3.22872967e-01, -7.14783370e-01,  1.10849380e-01, -2.92724103e-01,\n",
       "        2.03035951e-01, -4.89898413e-01,  6.31144524e-01, -6.21100776e-02,\n",
       "       -7.19894707e-01,  2.68211141e-02, -4.67553884e-02,  1.15437642e-01,\n",
       "        1.94763511e-01,  2.95969029e-03, -1.16841145e-01,  5.25401309e-02,\n",
       "       -3.26933742e-01, -3.77848089e-01,  5.70422232e-01,  2.70195007e-02,\n",
       "        1.25763625e-01,  1.93655834e-01,  1.16202153e-01,  4.85068232e-01,\n",
       "        4.84090656e-01, -6.30918086e-01, -2.13249311e-01, -2.20090330e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener el vector de una palabra\n",
    "w2v_model.wv['player']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9091a4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disciplin', 0.8215752840042114),\n",
       " ('teach', 0.7981538772583008),\n",
       " ('pupil', 0.770003616809845),\n",
       " ('educ', 0.7632160186767578),\n",
       " ('citizenship', 0.7582079768180847),\n",
       " ('nhs', 0.7455408573150635),\n",
       " ('social', 0.7404744029045105),\n",
       " ('disadvantag', 0.7298678755760193),\n",
       " ('lesson', 0.7138912677764893),\n",
       " ('democraci', 0.7130954265594482)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buscar palabras similares\n",
    "w2v_model.wv.most_similar('skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3381ae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'player'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar la palabra que no encaja en el conjunto\n",
    "w2v_model.wv.doesnt_match(['market', 'player', 'bank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e3665",
   "metadata": {},
   "source": [
    "## JUNIO: Clasificación de documentos\n",
    "### Naive-Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd5ebc19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluación del clasificador\u001b[39;00m\n\u001b[0;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(y_test, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparación de los datos\n",
    "count_vec = CountVectorizer()\n",
    "bow = count_vec.fit_transform(df['content'])\n",
    "bow = np.array(bow.todense())\n",
    "\n",
    "X = bow\n",
    "y = df['category']\n",
    "\n",
    "# Creación del conjunto de características\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y) \n",
    "\n",
    "# Entrenamiento del clasificador MultinomialNB\n",
    "mnb_classifier = MultinomialNB()\n",
    "model = mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación del clasificador\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8584b5",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "labels = set(df['category'].tolist())\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = labels)\n",
    "cm_display.plot(cmap='Oranges')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ba167",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Preparación de los datos\n",
    "X = df['content']  # Textos\n",
    "y = df['category']  # Etiquetas\n",
    "\n",
    "# Representación de los textos utilizando Word2Vec\n",
    "vector_size = w2v_model.vector_size\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = preprocess_text(text)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "# Creación del conjunto de características\n",
    "X = np.array([text_to_vector(text) for text in X])\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Entrenamiento del clasificador RandomForest\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 100) # n_estimators = int - Representa el número de árboles de decisión que se utilizarán en el conjunto de bosques aleatorios.\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación del clasificador\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586217d7",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65237814",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "labels = set(df['category'].tolist())\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = labels)\n",
    "cm_display.plot(cmap='Oranges')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
