{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e0e7a2",
   "metadata": {},
   "source": [
    "# Clasificación de documentos con word2vec\n",
    "### Dpto. Ciencias de la Computación e Inteligencia Artificial - Universidad de Sevilla\n",
    "Realizado por Lucas Antoñanzas del Villar (lucantdel) y Jaime García García (jaigagar1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36e028",
   "metadata": {},
   "source": [
    "## Construcción del corpus\n",
    "En este apartado, se describe el proceso de construcción del corpus utilizado en nuestro proyecto de clasificación de documentos. Encontramos una [compilación de 40 datasets de noticias gratis](https://www.linkedin.com/pulse/free-news-datasets-mega-compilation-rajat-thakur/) y tras contemplar varias opciones escogimos una [colección de textos periodísticos en inglés de la BBC](http://mlg.ucd.ie/datasets/bbc.html). Esta fuente nos proporciona una variedad de artículos de diferentes categorías, en concreto: business, entertainment, politics, sport y tech.\n",
    "\n",
    "Para obtener el corpus, accedimos al enlace proporcionado y descargamos los archivos de texto agrupados por categorías en diferentes carpetas. Cada carpeta contiene múltiples archivos .txt de una categoría específica, y cada uno de estos, un artículo (título, subtítulo y cuerpo de la noticia).\n",
    "\n",
    "Una vez obtenidos los textos de las diferentes categorías, procedimos a transformar todos estos archivos .txt a un solo archivo .csv con las siguientes columnas: id, article y category. Etiquetamos los artículos según la carpeta a la que pertenecen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb7ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated the .csv file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Directorio raíz donde se encuentran las carpetas por categoría\n",
    "root_directory = \"./bbc-fulltext/bbc\"\n",
    "\n",
    "# Nombre del archivo .csv de salida\n",
    "csv_file = \"./news.csv\"\n",
    "\n",
    "# Lista para almacenar las filas del archivo .csv\n",
    "csv_rows = []\n",
    "\n",
    "# Número por el que comenzarán los id\n",
    "id = 0\n",
    "\n",
    "# Recorre todas las carpetas en el directorio raíz\n",
    "for category in os.listdir(root_directory):\n",
    "    # Obtiene la ruta completa de la carpeta de la categoría\n",
    "    category_folder  = os.path.join(root_directory, category)\n",
    "    \n",
    "    # Verifica que sea una carpeta\n",
    "    if os.path.isdir(category_folder):\n",
    "        # Recorre todos los archivos .txt en la carpeta de la categoría\n",
    "        for txt_file  in os.listdir(category_folder):\n",
    "            # Obtiene la ruta completa del archivo .txt\n",
    "            txt_file_path  = os.path.join(category_folder, txt_file)\n",
    "            \n",
    "            # Lee el contenido del archivo .txt\n",
    "            with open(txt_file_path, 'r') as file:\n",
    "                content  = file.read().replace(\".\\n\\n\", \". \").replace(\"\\n\\n\", \". \").replace(\"\\n\", \"\")\n",
    "                \n",
    "                # Agrega una nueva fila al archivo .csv\n",
    "                row = [id, category, content]\n",
    "                csv_rows.append(row)\n",
    "                id += 1\n",
    "\n",
    "# Escribe las filas en el archivo .csv\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    csv_writer  = csv.writer(file, delimiter=';')\n",
    "    csv_writer.writerow(['id', 'category', 'content'])\n",
    "    csv_writer.writerows(csv_rows)\n",
    "\n",
    "print(\"Successfully generated the .csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192107b6",
   "metadata": {},
   "source": [
    "#### Análisis del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f62927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport            511\n",
      "business         510\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"news.csv\", sep=';')\n",
    "\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f993eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de filas: 2225\n"
     ]
    }
   ],
   "source": [
    "print(\"Número total de filas:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa1be7",
   "metadata": {},
   "source": [
    "A continuación se muestran 5 filas aleatorias del archivo .csv que hemos generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a646868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>business</td>\n",
       "      <td>Peugeot deal boosts Mitsubishi. Struggling Jap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>1854</td>\n",
       "      <td>tech</td>\n",
       "      <td>Solutions to net security fears. Fake bank e-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>227</td>\n",
       "      <td>business</td>\n",
       "      <td>Tsunami 'to hit Sri Lanka banks'. Sri Lanka's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>business</td>\n",
       "      <td>Market unfazed by Aurora setback. As the Auror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>541</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Redford's vision of Sundance. Despite sporting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       category                                            content\n",
       "12      12       business  Peugeot deal boosts Mitsubishi. Struggling Jap...\n",
       "1854  1854           tech  Solutions to net security fears. Fake bank e-m...\n",
       "227    227       business  Tsunami 'to hit Sri Lanka banks'. Sri Lanka's ...\n",
       "243    243       business  Market unfazed by Aurora setback. As the Auror...\n",
       "541    541  entertainment  Redford's vision of Sundance. Despite sporting..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_index = random.sample(range(0, df.shape[0]), 5)\n",
    "df.iloc[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f179d4",
   "metadata": {},
   "source": [
    "## Tokenización de texto y eliminación de 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecf07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Elimina todo lo que no sean letras y lo sustituye por espacios\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # Pasa las palabras a minúsculas y separa las palabras\n",
    "    tokens = review_text.lower().split()\n",
    "    \n",
    "    # Elimina los caracteres sueltos\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    # Elimina las stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d9499",
   "metadata": {},
   "source": [
    "A continuación, mostraremos un articulo del csv y el resultado de tokenizarlo y eliminar las stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6c482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "article = df['content'].tolist()[1]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b29cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tokenize_and_remove_stopwords(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b33aa",
   "metadata": {},
   "source": [
    "## Stemming con NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Definir el stemmer a utilizar\n",
    "stemmer = SnowballStemmer('english')  # Utiliza SnowballStemmer para español, puedes cambiarlo según el idioma\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenización: utilizamos la función creada en el apartado anterior\n",
    "    tokens = tokenize_and_remove_stopwords(text)\n",
    "    \n",
    "    # Aplicar stemming\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c89a3c",
   "metadata": {},
   "source": [
    "Este es el resultado de aplicar stemming a los tokens del articulo de ejemplo del apartado anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f99a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_text(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547865b9",
   "metadata": {},
   "source": [
    "A continuación, crearemos el vocabulario final en función de la frecuencia de las palabras que aparecen en el corpus preprocesado. Podemos establecer un umbral para filtrar las palabras de baja frecuencia y considerar solo aquellas que aparecen con mayor regularidad. Esta selección nos permitirá reducir la dimensionalidad y centrarnos en las palabras más informativas para la clasificación de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c046db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Definir un umbral para filtrar palabras de baja frecuencia\n",
    "threshold = 10\n",
    "\n",
    "# Creación del corpus y vocabulario de todos los artículos\n",
    "corpus = [preprocess_text(text) for text in df['content'].tolist()]\n",
    "all_words = [word for doc in corpus for word in doc]\n",
    "all_words_freq = Counter(all_words)\n",
    "vocabulary = [word for word, freq in all_words_freq.items() if freq >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb074f4",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec es un algoritmo de aprendizaje automático utilizado para representar palabras como vectores numéricos en un espacio vectorial. Se basa en la idea de que el significado de una palabra se puede capturar en función de su contexto lingüístico. Word2Vec utiliza una red neuronal para aprender a predecir la probabilidad de que una palabra aparezca en función de las palabras vecinas en un corpus de texto grande. A medida que el modelo se entrena en un corpus, las palabras similares tienden a tener vectores similares en el espacio vectorial. Estos vectores resultantes pueden usarse para medir la similitud entre palabras, realizar operaciones de analogía y mejorar el rendimiento en tareas de procesamiento de lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548f47b",
   "metadata": {},
   "source": [
    "#### Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Cuenta el número de núcleos de tu ordenador\n",
    "cores = multiprocessing.cpu_count() \n",
    "\n",
    "w2v_model = Word2Vec(min_count=threshold, # min_count = int - Ignora todas las palabras con una frecuencia absoluta total menor que esto - (2, 100)\n",
    "                 window=5, # window = int - La distancia máxima entre la palabra actual y la palabra predicha dentro de una oración. Por ejemplo, \"window\" palabras a la izquierda y \"window\" palabras a la derecha de nuestro objetivo - (2, 10)\n",
    "                 vector_size=200, # size = int - Dimensionalidad de los vectores de características - (50, 300)\n",
    "                 sample=1e-4, # sample = float - Umbral para configurar qué palabras de alta frecuencia se reducen aleatoriamente. Muy influyente - (0, 1e-5)\n",
    "                 alpha=0.03, # alpha = float - Tasa de aprendizaje inicial - (0.01, 0.05)\n",
    "                 min_alpha=0.0007, # min_alpha = float - La tasa de aprendizaje disminuirá linealmente hasta min_alpha a medida que avance el entrenamiento. Para establecerlo: alpha - (min_alpha * epochs) ~ 0.00\n",
    "                 negative=10, # negative = int - Si es > 0, se utilizará muestreo negativo, donde el entero para negative especifica cuántas \"palabras de ruido\" se deben seleccionar. Si se establece en 0, no se utiliza muestreo negativo - (5, 20)\n",
    "                 workers=cores-1) # workers = int - Utiliza esta cantidad de hilos de trabajo para entrenar el modelo (entrenamiento más rápido con máquinas multicore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af241a6",
   "metadata": {},
   "source": [
    "#### Construcción del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2819e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d91818",
   "metadata": {},
   "source": [
    "#### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d39731",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(corpus, \n",
    "            total_examples=w2v_model.corpus_count, # total_examples = int - Contador de oraciones\n",
    "            epochs=20) # epochs = int - Número de iteraciones (épocas) sobre el corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda276e",
   "metadata": {},
   "source": [
    "#### Ejemplos\n",
    "WARNING: Las palabras introducidas como parámetro deben estar contenidas en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327dd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el vector de una palabra\n",
    "w2v_model.wv['player']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar palabras similares\n",
    "w2v_model.wv.most_similar('skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la palabra que no encaja en el conjunto\n",
    "w2v_model.wv.doesnt_match(['market', 'player', 'bank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e3665",
   "metadata": {},
   "source": [
    "## JUNIO: Clasificación de documentos\n",
    "### Naive-Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ebc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparación de los datos\n",
    "count_vec = CountVectorizer()\n",
    "bow = count_vec.fit_transform(df['content'])\n",
    "bow = np.array(bow.todense())\n",
    "\n",
    "X = bow\n",
    "y = df['category']\n",
    "\n",
    "# Creación del conjunto de características\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y) \n",
    "\n",
    "# Entrenamiento del clasificador MultinomialNB\n",
    "mnb_classifier = MultinomialNB()\n",
    "model = mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación del clasificador\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8584b5",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "labels = set(df['category'].tolist())\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = labels)\n",
    "cm_display.plot(cmap='Oranges')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ba167",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Preparación de los datos\n",
    "X = df['content']  # Textos\n",
    "y = df['category']  # Etiquetas\n",
    "\n",
    "# Representación de los textos utilizando Word2Vec\n",
    "vector_size = w2v_model.vector_size\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = preprocess_text(text)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "# Creación del conjunto de características\n",
    "X = np.array([text_to_vector(text) for text in X])\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Entrenamiento del clasificador RandomForest\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 100) # n_estimators = int - Representa el número de árboles de decisión que se utilizarán en el conjunto de bosques aleatorios.\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación del clasificador\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586217d7",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65237814",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "labels = set(df['category'].tolist())\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = labels)\n",
    "cm_display.plot(cmap='Oranges')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
